\thispagestyle{empty}
\newgeometry{top=1.75in}
\begin{center}
    ANALYZING REPETITIVE SEQUENCES WITH STRUCTURED DYNAMIC BAYESIAN NETWORKS

    \vspace{3\baselineskip}
    Thomas L. Lake, M.S.

    \vspace{\baselineskip}
    Western Michigan University, 2015
\end{center}
\vspace{3\baselineskip}
\hspace*{0.5in}
Time series often feature structure that is known a priori and easily described using
natural language terms such as repetitive, symmetric, seasonal, and self-similar.
However, the typical conjugate priors used in Bayesian analysis do not capture such
complex phenomena well.
As a result of this mismatch, known structure is modeled poorly or completely ignored.
Focusing on time series with repetitive structure, this thesis proposes to overcome this
problem by reducing rather than increasing the capacity of a well know time series model,
the Hidden Markov Model.
Through a careful choice in the way model capacity is reduced the model is forced to use
its latent variables in an interpretable way which accurately reflects known structure.
In addition to increased modeling performance, the lower capacity model admits reduced
complexity inference and parameter estimation procedures.
Experimental results are presented demonstrating the effectiveness of this approach
in both supervised and unsupervised learning contexts.
\newpage
\restoregeometry
